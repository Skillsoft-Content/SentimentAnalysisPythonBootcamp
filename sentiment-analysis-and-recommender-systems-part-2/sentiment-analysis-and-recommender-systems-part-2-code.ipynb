{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################\n",
    "#######################################################\n",
    "############    COPYRIGHT - DATA SOCIETY   ############\n",
    "#######################################################\n",
    "#######################################################\n",
    "\n",
    "## SENTIMENT ANALYSIS AND RECOMMENDER SYSTEMS PART 2/SENTIMENT ANALYSIS AND RECOMMENDER SYSTEMS PART 2 ##\n",
    "\n",
    "## NOTE: To run individual pieces of code, select the line of code and\n",
    "##       press ctrl + enter for PCs or command + enter for Macs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 6: Directory settings  ####\n",
    "\n",
    "# Set 'main_dir' to location of the project folder\n",
    "from pathlib import Path\n",
    "home_dir = Path(\".\").resolve()\n",
    "main_dir = home_dir.parent\n",
    "data_dir = str(main_dir) + \"/data\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 7: Loading packages  ####\n",
    "\n",
    "# Helper packages.\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "# Packages with tools for text processing.\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "# Packages for working with text data and analyzing sentiment\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# Packages to build and measure the performance of a logistic regression model \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 8: Import data we saved   ####\n",
    "\n",
    "# Load pickled data and models.\n",
    "score_labels = pickle.load(open(data_dir + \"/score_labels.sav\",\"rb\"))\n",
    "DTM_matrix = pickle.load(open(data_dir + '/DTM_matrix.sav',\"rb\"))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 9: Text classification - convert DTM to array  ####\n",
    "\n",
    "DTM_array = DTM_matrix.toarray()\n",
    "# Let's look at the first few rows of the finalized array. \n",
    "print(DTM_array[1:4])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 13: Model building - split the dataset  ####\n",
    "\n",
    "X_train, X_test, y_train, y_test  = train_test_split(\n",
    "        DTM_array, \n",
    "        score_labels,\n",
    "        train_size = 0.70, \n",
    "        random_state = 1234)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 14: Model building - split the dataset  ####\n",
    "\n",
    "print(len(X_train))\n",
    "print(len(X_test))\n",
    "print(len(y_train))\n",
    "print(len(y_test))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 24: Categorical to binary target variable  ####\n",
    "\n",
    "# Initiate the Label Binarizer.\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "\n",
    "# Convert y_test to binary integer format. \n",
    "y_test= lb.fit_transform(y_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 31: Logistic regression: build  ####\n",
    "\n",
    "# Set up logistic regression model.\n",
    "log_model = LogisticRegression()\n",
    "print(log_model)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 32: Logistic regression: fit  ####\n",
    "\n",
    "# Fit the model.\n",
    "log_model = log_model.fit(X = X_train, y = y_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 34: Logistic regression: predict (cont'd)  ####\n",
    "\n",
    "# Predict on test data.\n",
    "y_pred = log_model.predict(X_test)\n",
    "print(y_pred)\n",
    "# Convert y_pred to binary integer format. \n",
    "y_pred= lb.fit_transform(y_pred)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 36: Exercise 1  ####\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 43: Confusion matrix and accuracy  ####\n",
    "\n",
    "# Take a look at test data confusion matrix.\n",
    "conf_matrix_test = metrics.confusion_matrix(y_test, y_pred)\n",
    "print(conf_matrix_test)\n",
    "# Compute test model accuracy score.\n",
    "test_accuracy_score = metrics.accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy on test data: \", test_accuracy_score)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 44: Classification report  ####\n",
    "\n",
    "# Create a list of target names to interpret class assignments.\n",
    "target_names = ['Negative', 'Positive']\n",
    "# Print an entire classification report.\n",
    "class_report = metrics.classification_report(y_test, \n",
    "                                             y_pred, \n",
    "                                             target_names = target_names)\n",
    "print(class_report)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 45: Classification report (cont'd)  ####\n",
    "\n",
    "print(class_report)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 46: Getting probabilities instead of class labels  ####\n",
    "\n",
    "# Get probabilities instead of predicted values.\n",
    "test_probabilities = log_model.predict_proba(X_test)\n",
    "print(test_probabilities[0:5, :])\n",
    "# Get probabilities of test predictions only.\n",
    "test_predictions = test_probabilities[: , 1]\n",
    "print(test_probabilities[0:5])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 47: Computing FPR, TPR and threshold  ####\n",
    "\n",
    "# Get FPR, TPR and threshold values.\n",
    "fpr, tpr, threshold = metrics.roc_curve(y_test,           #<- test data labels\n",
    "                                        test_predictions) #<- predicted probabilities\n",
    "print(\"False positive: \", fpr)\n",
    "print(\"True positive: \", tpr)\n",
    "print(\"Threshold: \", threshold)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 48: Computing AUC  ####\n",
    "\n",
    "# Get AUC value\n",
    "auc = metrics.roc_auc_score(y_test,y_pred)\n",
    "print(\"Area under the ROC curve: \", auc)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 49: Putting it all together: ROC plot  ####\n",
    "\n",
    "# Make an ROC curve plot.\n",
    "_=plt.title('Receiver Operator Characteristic')\n",
    "_=plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % auc)\n",
    "_=plt.legend(loc = 'lower right')\n",
    "_=plt.plot([0, 1], [0, 1],'r--')\n",
    "_=plt.xlabel('False Positive Rate')\n",
    "_=plt.ylabel('True Positive Rate')\n",
    "_=plt.show()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
