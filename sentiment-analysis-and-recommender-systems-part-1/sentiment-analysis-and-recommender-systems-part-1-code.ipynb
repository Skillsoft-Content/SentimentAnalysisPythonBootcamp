{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################\n",
    "#######################################################\n",
    "############    COPYRIGHT - DATA SOCIETY   ############\n",
    "#######################################################\n",
    "#######################################################\n",
    "\n",
    "## SENTIMENT ANALYSIS AND RECOMMENDER SYSTEMS PART 1/SENTIMENT ANALYSIS AND RECOMMENDER SYSTEMS PART 1 ##\n",
    "\n",
    "## NOTE: To run individual pieces of code, select the line of code and\n",
    "##       press ctrl + enter for PCs or command + enter for Macs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 9: Directory settings  ####\n",
    "\n",
    "# Set 'main_dir' to location of the project folder\n",
    "from pathlib import Path\n",
    "home_dir = Path(\".\").resolve()\n",
    "main_dir = home_dir.parent\n",
    "data_dir = str(main_dir) + \"/data\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 10: Loading packages  ####\n",
    "\n",
    "# Helper packages.\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "# Packages for working with text data and analyzing sentiment\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#set up nltk packages\n",
    "import nltk\n",
    "nltk.download('all')\n",
    "import nltk.data\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem.porter import PorterStemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 15: Load text data  ####\n",
    "\n",
    "# Load corpus from a csv (for Mac).\n",
    "NYT = pd.read_csv(data_dir + '/NYT_article_data.csv')\n",
    "print(NYT.columns)\n",
    "# Isolate the snippet column.\n",
    "NYT_snippet = NYT[\"snippet\"]\n",
    "# Look at a sample of the snippets column, 0-10.\n",
    "print(NYT[\"snippet\"][0:10])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 16: Tokenization: split each snippet into words  ####\n",
    "\n",
    "# Tokenize each snippet into a large list of tokenized snippets.\n",
    "NYT_tokenized = [word_tokenize(NYT_snippet[i]) for i in range(0, len(NYT_snippet))]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 18: Implementing pre-processing steps on a corpus  ####\n",
    "\n",
    "# Create a list for clean snippets.\n",
    "NYT_clean = [None] * len(NYT_tokenized)\n",
    "# Create a list of word counts for each clean snippet.\n",
    "word_counts_per_snippet = [None] * len(NYT_tokenized)\n",
    "# Process words in all snippets.\n",
    "for i in range(len(NYT_tokenized)):\n",
    "    # 1. Convert to lower case.\n",
    "    NYT_clean[i] = [snippet.lower() for snippet in NYT_tokenized[i]]\n",
    "    \n",
    "    # 2. Remove stop words.\n",
    "    stop_words = stopwords.words('english')\n",
    "    NYT_clean[i] = [word for word in NYT_clean[i] if not word in stop_words]\n",
    "    \n",
    "    # 3. Remove punctuation and any non-alphabetical characters.\n",
    "    NYT_clean[i] = [word for word in NYT_clean[i] if word.isalpha()]\n",
    "    \n",
    "    # 4. Stem words.\n",
    "    NYT_clean[i] = [PorterStemmer().stem(word) for word in NYT_clean[i]]\n",
    "    \n",
    "    # Record the word count per snippet.\n",
    "    word_counts_per_snippet[i] = len(NYT_clean[i])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 19: Inspect results  ####\n",
    "\n",
    "print(NYT_clean[0][:10])\n",
    "print(NYT_clean[5][:10])\n",
    "print(NYT_clean[10][:10])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 20: Removing empty and very short snippets  ####\n",
    "\n",
    "print(word_counts_per_snippet[:10])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 21: Removing empty and very short snippets (cont'd)  ####\n",
    "\n",
    "# Convert word counts list and snippets list to numpy arrays.\n",
    "word_counts_array = np.array(word_counts_per_snippet)\n",
    "NYT_array = np.array(NYT_clean, dtype=object)\n",
    "print(len(NYT_array))\n",
    "# Find indices of all snippets where there are greater than or equal to 5 words.\n",
    "valid_snippets = np.where(word_counts_array >= 5)[0]\n",
    "print(len(valid_snippets))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 22: Removing empty and very short snippets (cont'd)  ####\n",
    "\n",
    "# Subset the NYT_array to keep only those where there are at least 5 words.\n",
    "NYT_array = NYT_array[valid_snippets]\n",
    "print(len(NYT_array))\n",
    "\n",
    "# Convert the array back to a list.\n",
    "NYT_clean = NYT_array.tolist()\n",
    "print(NYT_clean[:3])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 23: Save processed text to file using .join()  ####\n",
    "\n",
    "# Join words in each snippet into a single character string.\n",
    "NYT_clean_list = [' '.join(snippet) for snippet in NYT_clean]\n",
    "print(NYT_clean_list[:5])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 26: Create a DTM  ####\n",
    "\n",
    "# Initialize `CountVectorizer`.\n",
    "vec = CountVectorizer()\n",
    "\n",
    "# Transform the list of snippets into DTM.\n",
    "X = vec.fit_transform(NYT_clean_list)\n",
    "print(X.toarray()) #<- to show output as a matrix\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 32: Exercise 1  ####\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 38: Text classification - classify (cont'd)  ####\n",
    "\n",
    "# Initialize the `SentimentIntensityAnalyzer().`\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Iterate through each sentence printing out the scores for each.\n",
    "for sentence in NYT_clean_list[:5]:\n",
    "     print(sentence)\n",
    "     ss = sid.polarity_scores(sentence)\n",
    "     for k in ss:\n",
    "         print('{0}: {1}, '.format(k, ss[k]), end='')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 40: Text classification - classify (cont'd)  ####\n",
    "\n",
    "# This function outputs a list of labels for snippet:\n",
    "def sentiment_analysis(texts):\n",
    "        list_of_scores = []\n",
    "        for text in texts:\n",
    "            sid = SentimentIntensityAnalyzer()               \n",
    "            compound = sid.polarity_scores(text)[\"compound\"] \n",
    "            if compound >= 0:\n",
    "                list_of_scores.append(\"positive\")\n",
    "            else:\n",
    "                list_of_scores.append(\"negative\")\n",
    "        return(list_of_scores)\n",
    "score_labels = sentiment_analysis(NYT_clean_list)\n",
    "print(score_labels[1:5])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=================================================-\n",
    "#### Slide 42: Save results as a pickle  ####\n",
    "\n",
    "pickle.dump(NYT_clean_list, open(data_dir + '/NYT_clean_list.sav', 'wb'))\n",
    "pickle.dump(score_labels, open(data_dir + '/score_labels.sav', 'wb'))\n",
    "pickle.dump(X, open(data_dir + '/DTM_matrix.sav', 'wb'))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
