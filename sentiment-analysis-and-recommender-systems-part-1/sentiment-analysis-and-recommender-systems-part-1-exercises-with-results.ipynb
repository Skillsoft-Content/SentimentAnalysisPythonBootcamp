{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis and Recommender Systems Part 1 - Exercises with Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following exercises, we will work with data from movie reviews for the sentiment analysis. The movie reviews were scraped from a website. Each review is a document and, collectively, the reviews form the corpus. We will be using the `movie_reviews.csv` file today!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1\n",
    "##### Load the following packages/libraries that are used in this module:\n",
    "##### pandas, numpy, pickle (Helper packages); nltk (natural language toolkit for text processing); scikit-learn; matplotlib (for visualizing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T15:04:42.785250Z",
     "start_time": "2020-06-22T15:04:41.081119Z"
    }
   },
   "outputs": [],
   "source": [
    "# Helper packages.\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# Packages with tools for text processing.\n",
    "import nltk\n",
    "\n",
    "# Packages to clean text data\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Packages for working with text data and analyzing sentiment.\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2\n",
    "##### Set working directory to folder where the dataset is present."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "home_dir = Path(\".\").resolve()\n",
    "main_dir = home_dir.parent\n",
    "\n",
    "data_dir = str(main_dir) + \"/data\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3\n",
    "\n",
    "##### Load the `movie_reviews.csv` file and preview the data.\n",
    "##### Execute the below chunk of code that performs the following steps in order to clean the movie reviews data.\n",
    "\n",
    "    1. Convert all characters to lower case \n",
    "    2. Remove stop words \n",
    "    3. Remove punctuation, numbers, and all other symbols that are not letters \n",
    "    4. Stem words\n",
    "    5. Filter the reviews which have greater than three words.\n",
    "    6. Save the cleaned reviews in the list `reviews_clean_list` and \n",
    "    7. Create a Document-Term Matrix and save it as `reviews_DTM_array.` \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_reviews = pd.read_csv(data_dir + '/movie_reviews.csv')\n",
    "movie_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = movie_reviews[\"reviews\"]\n",
    "\n",
    "reviews_tokenized = [word_tokenize(reviews[i]) for i in range(0,len(reviews))]\n",
    "\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# Create a vector for clean reviews.\n",
    "reviews_clean = [None] * len(reviews_tokenized)\n",
    "\n",
    "# Create a vector of word counts for each clean reviews.\n",
    "word_counts_per_review = [None] * len(reviews_tokenized)\n",
    "\n",
    "# Process words in all documents.\n",
    "for i in range(len(reviews_tokenized)):\n",
    "    # 1. Convert to lower case.\n",
    "    reviews_clean[i] = [reviews.lower() for reviews in reviews_tokenized[i]]\n",
    "    \n",
    "    # 2. Remove stopwords.\n",
    "    reviews_clean[i] = [word for word in reviews_clean[i] if not word in stop_words]\n",
    "    \n",
    "    # 3. Remove punctuation and any non-alphabetical characters.\n",
    "    reviews_clean[i] = [word for word in reviews_clean[i] if word.isalpha()]\n",
    "    \n",
    "    # 4. Stem words.\n",
    "    reviews_clean[i] = [PorterStemmer().stem(word) for word in reviews_clean[i]]\n",
    "    \n",
    "    # Record the word count per reviews.\n",
    "    word_counts_per_review[i] = len(reviews_clean[i])\n",
    "    \n",
    "# Array with length of each titles.\n",
    "ex_word_counts_array = np.array(word_counts_per_review)\n",
    "reviews_array = np.array(reviews_clean, dtype=object)\n",
    "print(len(reviews_array))\n",
    "\n",
    "# Find indices of all reviews where there are at least 3 words.\n",
    "valid_titles = np.where(ex_word_counts_array >= 3)[0]\n",
    "\n",
    "# Subset the reviews array to keep only those where there are at least 3 words.\n",
    "reviews_array = reviews_array[valid_titles]\n",
    "print(len(reviews_array))\n",
    "\n",
    "# Convert the array back to a list.\n",
    "reviews_clean = reviews_array.tolist()\n",
    "    \n",
    "reviews_clean_list = [' '.join(message) for message in reviews_clean]\n",
    "\n",
    "ex_vec = CountVectorizer()\n",
    "ex_X = ex_vec.fit_transform(reviews_clean_list)\n",
    "\n",
    "reviews_DTM_array = ex_X.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4\n",
    "\n",
    "##### Print the first 10 reviews from `reviews_clean_list`.\n",
    "\n",
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reviews_clean_list[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1\n",
    "##### We want to analyze the sentiment of the movie reviews.\n",
    "##### Let us first add the sentiment labels to our cleaned reviews.\n",
    "##### Load the sentiment analysis function we used in our module.\n",
    "\n",
    "##### This function outputs a list of labels for each chat message:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T15:05:06.580778Z",
     "start_time": "2020-06-22T15:05:06.573117Z"
    }
   },
   "outputs": [],
   "source": [
    "def sentiment_analysis(texts):\n",
    "    list_of_scores = []\n",
    "    for text in texts:\n",
    "        sid = SentimentIntensityAnalyzer()\n",
    "        compound = sid.polarity_scores(text)[\"compound\"]\n",
    "        if compound >= 0:\n",
    "            list_of_scores.append(\"positive\")\n",
    "        else:\n",
    "            list_of_scores.append(\"negative\")\n",
    "    return(list_of_scores) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2\n",
    "##### Assign labels to the `reviews_clean_list` using the `sentiment_analysis` function and save to them to `score_labels` variable.\n",
    "##### Print the first 5 labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T15:05:10.254753Z",
     "start_time": "2020-06-22T15:05:07.696595Z"
    }
   },
   "outputs": [],
   "source": [
    "score_labels = sentiment_analysis(reviews_clean_list)\n",
    "\n",
    "print(score_labels[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3\n",
    "##### Save `score_labels`  as `score_labels_ex.sav`, `reviews_clean_list` as `reviews_clean_list_ex.sav`, and `reviews_DTM_array` as `reviews_DTM_array.sav` in the data_dir for using in next session.\n",
    "\n",
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(score_labels, open(data_dir + '/score_labels_ex.sav', 'wb'))\n",
    "pickle.dump(reviews_clean_list, open(data_dir + '/reviews_clean_list_ex.sav', 'wb'))\n",
    "pickle.dump(reviews_DTM_array, open(data_dir + '/reviews_DTM_array.sav', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "af1c2328f5544355579310f76569f81b262cdac47b296e1f8c8cfd250fec34f9"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
